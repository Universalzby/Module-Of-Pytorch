{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"../../Problems of Data/Car insurance risk\"\n",
    "train_path = \"/train.csv\"\n",
    "test_path = \"/test.csv\"\n",
    "submission_path = \"/submission.csv\"\n",
    "init_epochs = 10\n",
    "init_batch_size = 128\n",
    "init_learning_rate = 1e-4\n",
    "init_momentum = 0.99\n",
    "init_feature_size = 111"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画图 https://matplotlib.org/api/animation_api.html\n",
    "1. matplotlib.animation.FuncAnimation(\n",
    "    fig, \n",
    "    animate, \n",
    "    init_func=init, \n",
    "    frames=100, \n",
    "    interval=20, \n",
    "    blit=true) 函数可以实现动态画图\n",
    "2. fig：画布, animate: 随时间变化的数值函数, init_func: 初始化设定, frames:时间t的个数, interval:图的刷新频率, blit: 只修改变化的部分，加快绘图速度\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib.animation import FuncAnimation\n",
    "# import seaborn as sns # 美化图形\n",
    "# fig, ax = plt.subplots()\n",
    "# def init():\n",
    "#     pass\n",
    "# ani = FuncAnimation(fig=fig, )\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path): # 注意异常处理\n",
    "    if os.path.exists:\n",
    "        return pd.read_csv(path)\n",
    "    else:\n",
    "        print(\"error： the file path is not exist！\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tips：处理特征\n",
    "1. 对于数值数据，需要归一化处理\n",
    "2. 对于非数值数据，需要进行编码处理 （one-hot编码）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def features_onehot(data): # 对pd.dataframe类型数据处理\n",
    "    if type(data) is not pd.core.frame.DataFrame:\n",
    "        print(\"数据格式不正确，无法处理数据!\")\n",
    "        return None\n",
    "    cols = [x for x in data.columns if type(data[x][0]) is str]\n",
    "    data = pd.get_dummies(data, columns=cols)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义线性模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.lr = nn.Sequential(\n",
    "            nn.Linear(input_size, 1) # 线性回归是特殊的神经网络\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.lr(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集整理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据集\n",
    "class Get_Dataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.data = features_onehot(read_data(path)).values\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index][2:], self.data[index][1]\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "# 获取数据集和批数据集\n",
    "def get_data(path):\n",
    "    train_dataset = Get_Dataset(path)\n",
    "    train_load = DataLoader(train_dataset, shuffle=True, batch_size=init_batch_size)\n",
    "    return train_dataset, train_load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "if torch.cuda.is_available:\n",
    "    model = LinearRegression(init_feature_size).cuda()\n",
    "else:\n",
    "    model = LinearRegression(init_feature_size)\n",
    "# 定义损失函数\n",
    "loss_function = nn.MSELoss(reduction=\"sum\") # 不取平均\n",
    "# 定义优化策略\n",
    "optimizer = optim.SGD(model.parameters(), lr=init_learning_rate, momentum=init_momentum)\n",
    "# 加载训练数据集\n",
    "train_dataset, train_load = get_data(dir_path+train_path)\n",
    "# 加载测试数据\n",
    "test_dataset,_ = get_data(dir_path+test_path)\n",
    "# len(train_load.__iter__().__next__()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "tensor([[nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([ 1., 10.,  5.,  4.,  5.,  1.,  5.,  1.,  1.,  4.,  4.,  1., 15., 18.,\n",
      "         4.,  1.,  5.,  1.,  7., 19.,  1.,  1.,  1.,  1.,  1.,  1.,  2.,  4.,\n",
      "         4.,  4.,  1.,  4.,  4., 10.,  4.,  5., 12.,  1.,  4.,  1.,  2.,  8.,\n",
      "        10.,  9.,  4., 10.,  2.,  1.,  4.,  1.,  1.,  7.,  1.,  7.,  2.,  8.,\n",
      "         1., 11.,  2.,  1., 42.,  8.,  4.,  7.,  1.,  1.,  3.,  1.,  1.,  1.,\n",
      "         7., 18.,  7.,  6.,  1.,  5., 14.,  4.,  8.,  1.,  4.,  5.,  5.,  2.,\n",
      "         4.,  1.,  5., 10.,  2.,  2.,  4.,  1.,  4.,  1.,  1.,  2., 10.,  8.,\n",
      "         6.,  3.,  1., 12.,  1.,  5.,  3.,  1.,  7., 12.,  1.,  4.,  7.,  1.,\n",
      "         2.,  4.,  3.,  9.,  5.,  2.,  2.,  3.,  7.,  3.,  1.,  1.,  9.,  8.,\n",
      "         4., 10.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for epoch in range(init_epochs):\n",
    "    last_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    print(\"*\"*20)\n",
    "    for (i, data) in enumerate(train_load, 1):\n",
    "        feature, score = data\n",
    "        \n",
    "        if torch.cuda.is_available:\n",
    "            feature, score = Variable(feature).cuda(), Variable(score).cuda()\n",
    "        else:\n",
    "            feature, score = Variable(feature), Variable(score)\n",
    "        feature, score = feature.float(), score.float()\n",
    "        out = model(feature)\n",
    "        loss = loss_function(out, score)\n",
    "        print(out, score)\n",
    "        break\n",
    "        train_loss += loss.data\n",
    "        train_acc += (torch.abs(out - score) <=1).sum().item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i%20 == 0:\n",
    "            print(\"times is {}\".format(time.time() - last_time))\n",
    "            print(\"[{}/{}]: loss is {}, acc is {}\".format(epoch + 1, init_epochs,\n",
    "                                                          train_loss, train_acc*100 / len(train_dataset)))\n",
    "            print(\"*\"*20)\n",
    "    break    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "xdata, ydata = [], []\n",
    "ln, = plt.plot([], [], 'ro', animated=True)\n",
    "\n",
    "def init():\n",
    "    ax.set_xlim(0, 100)\n",
    "    ax.set_ylim(-1, 1)\n",
    "    return ln,\n",
    "\n",
    "def update(frame):\n",
    "    xdata.append(frame)\n",
    "    ydata.append(np.sin(frame))\n",
    "    ln.set_data(xdata, ydata)\n",
    "    return ln,\n",
    "\n",
    "ani = FuncAnimation(fig, update, frames=np.linspace(0, 100, 10000),interval=0,\n",
    "                    init_func=init, blit=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
